{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlx_parallm\n",
    "# import importlib\n",
    "# importlib.reload(mlx_parallm)\n",
    "from utils import load, generate, batch_generate\n",
    "import string\n",
    "import random\n",
    "\n",
    "# load model\n",
    "#model, tokenizer = load(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "#model, tokenizer = load(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "#model, tokenizer = load(\"google/gemma-1.1-2b-it\")\n",
    "model, tokenizer = load(\"mlx-community/Meta-Llama-3-8B-Instruct-4bit\")\n",
    "#model, tokenizer = load(\"mlx-community/Phi-3-mini-4k-instruct-4bit\")\n",
    "#model, tokenizer = load(\"mlx-community/gemma-1.1-2b-it-4bit\")\n",
    "\n",
    "\n",
    "capital_letters = string.ascii_uppercase\n",
    "distinct_pairs = [(a, b) for i, a in enumerate(capital_letters) for b in capital_letters[i + 1:]]\n",
    "\n",
    "num_prompts = 325\n",
    "prompt_template = \"Think of a real word containing both the letters {l1} and {l2}. Then, say 3 sentences which use the word.\"\n",
    "prompts_raw = [prompt_template.format(l1=p[0], l2=p[1]) for p in random.sample(distinct_pairs, num_prompts)]\n",
    "prompt_template_2 = \"Come up with a real English word containing both the letters {l1} and {l2}. No acronyms. Then, give 3 complete sentences which use the word.\"\n",
    "prompts_raw_2 = [prompt_template_2.format(l1=p[0], l2=p[1]) for p in random.sample(distinct_pairs, num_prompts)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = batch_generate(model, tokenizer, prompts=prompts_raw[:10]+prompts_raw_2[:10], max_tokens=100, verbose=True, temp=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = generate(model, tokenizer, prompt=prompts_raw[0], max_tokens=100, verbose=True, temp=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = batch_generate(model, tokenizer, prompts=prompts_raw, max_tokens=100, verbose=True, format_prompts=True, temp=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = batch_generate(model, tokenizer, prompts=prompts_raw[:10]+prompts_raw_2[:10], max_tokens=100, verbose=True, temp=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLX Batch Generator (uv)",
   "language": "python",
   "name": "mlx-batch-generator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
